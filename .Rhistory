# Visualisation des individus dans l'espace des deux premières composantes principales
plotIndiv(plsda_res, comp = c(1,3), group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE)
# Visualisation des individus dans l'espace des deux premières composantes principales
plotIndiv(plsda_res, comp = c(1,2,3), group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title =" PLS-DA on comp")
# Visualisation des individus dans l'espace des deux premières composantes principales
plotIndiv(plsda_res, comp = c(1,3), group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title =" PLS-DA on comp")
# Visualisation des individus dans l'espace des deux premières composantes principales
plotIndiv(plsda_res, comp = c(1,3), group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title =" PLS-DA")
# Visualisation des individus dans l'espace des deux premières composantes principales
plotIndiv(plsda_res, comp = c(1,3), group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title =" PLS-DA sur trois composantes")
# Prédiction des classes des individus de l'échantillon de test
X_test <- as.matrix(df_test[,2:257])
Y_test_pred <- predict(plsda_res, newdata = X_test)$class
# Calcul du taux de bonne classification
mean(Y_test_pred[["mahalanobis.dist"]] == df_test[,1])
# Séparation des données en matrices X et Y
X_train <- as.matrix(df_train[,2:257])
Y_train <- as.factor(df_train[,1])
# Réalisation de l'analyse PLS-DA
plsda_res <- plsda(X_train, Y_train, ncomp = 5)
plotIndiv(plsda_res, group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title =" PLS-DA sur cinq composantes")
# Calcul du taux de bonne classification
mean(Y_test_pred$class == df_test[,1])
Y_test_pred$max.dist
plotLoadings(plsda_res, comp = 3, method = 'median', contrib = "max")
plotLoadings(plsda_res, comp = 1, method = 'median', contrib = "max")
plotLoadings(plsda_res, comp = 2, method = 'median', contrib = "max")
plotLoadings(plsda_res, comp = 3, method = 'median', contrib = "max")
plotLoadings(plsda_res, comp = 1, method = 'median', contrib = "max")
plotLoadings(plsda_res, comp = 2, method = 'median', contrib = "max")
plotLoadings(plsda_res, comp = 3, method = 'median', contrib = "max")
plotLoadings(plsda_res, comp = 2, method = 'median', contrib = "max")
getConfusionMatrix(plsda_res)
getConfusionMatrix(Y_test_pred[["mahalanobis.dist"]] == df_test[,1])
getConfusionMatrix(Y_test_pred)
test.predict <- predict(plsda_res, X_test, dist = "max.dist")
Prediction <- test.predict$class$max.dist[, 2]
getConfusionMatrix(Y_test, predicted = Prediction)
getConfusionMatrix(truth = Y_test, predicted = Prediction)
get.confusion_matrix(truth = Y_test, predicted = Prediction)
View(Y_test_pred)
get.confusion_matrix(truth = Y_test_pred[["mahalanobis.dist"]], predicted = Prediction)
get.confusion_matrix(truth = Y_train, predicted = Prediction)
get.confusion_matrix(truth = Y_train, predicted = Prediction)
Y_train <- as.factor(df_train[,1])
get.confusion_matrix(truth = Y_train, predicted = Prediction)
get.confusion_matrix(truth = y_test, predicted = Prediction)
y_test = as.character(y_test)
get.confusion_matrix(truth = y_test, predicted = Prediction)
test.predict <- predict(plsda_res, X_test, dist = "max.dist")
Prediction <- test.predict$class$max.dist[, 2]
y_test = as.character(y_test)
get.confusion_matrix(truth = y_test, predicted = Prediction)
Y_test = as.character(Y_test)
# Préparation des données (assurez-vous que les données sont déjà divisées en ensembles d'apprentissage et de test)
X_train <- as.matrix(train_set[, 2:257])
y_train <- train_set[, 1]
X_test <- as.matrix(test_set[, 2:257])
y_test <- test_set[, 1]
get.confusion_matrix(truth = y_test, predicted = Prediction)
y_test <- df_test[, 1]
get.confusion_matrix(truth = y_test, predicted = Prediction)
get.BER(confusion.mat)
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
Y_test <- df_test[, 1]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
get.BER(confusion.mat)
# Réalisation de l'analyse PLS-DA
plsda_res <- plsda(X_train, Y_train, ncomp = 5)
# Séparation des données en matrices X et Y
X_train <- as.matrix(df_train[,2:257])
Y_train <- as.factor(df_train[,1])
y_test <- df_test[, 1]
# Réalisation de l'analyse PLS-DA
plsda_res <- plsda(X_train, Y_train, ncomp = 5)
plotIndiv(plsda_res, group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title = "PLS-DA sur cinq composantes")
# Réalisation de l'analyse PLS-DA
plsda_res <- plsda(X_train, Y_train, ncomp = 3)
# Visualisation des individus dans l'espace des trois premières composantes principales
plotIndiv(plsda_res, comp = c(1,3), group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title =" PLS-DA sur trois composantes")
# Prédiction des classes des individus de l'échantillon de test
X_test <- as.matrix(df_test[,2:257])
Y_test <- df_test[, 1]
Y_test_pred <- predict(plsda_res, newdata = X_test)$class
# Calcul du taux de bonne classification
mean(Y_test_pred[["mahalanobis.dist"]] == df_test[,1])
test.predict <- predict(plsda_res, X_test, dist = "max.dist")
Prediction <- test.predict$class$max.dist[, 2]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
get.BER(confusion.mat)
# Réalisation de l'analyse PLS-DA
plsda_res3 <- plsda(X_train, Y_train, ncomp = 3)
# Visualisation des individus dans l'espace des trois premières composantes principales
plotIndiv(plsda_res3, comp = c(1,3), group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title =" PLS-DA sur trois composantes")
# Prédiction des classes des individus de l'échantillon de test
X_test <- as.matrix(df_test[,2:257])
Y_test <- df_test[, 1]
Y_test_pred <- predict(plsda_res3, newdata = X_test)$class
# Calcul du taux de bonne classification
mean(Y_test_pred[["mahalanobis.dist"]] == df_test[,1])
# Séparation des données en matrices X et Y
X_train <- as.matrix(df_train[,2:257])
Y_train <- as.factor(df_train[,1])
# Réalisation de l'analyse PLS-DA
plsda_res <- plsda(X_train, Y_train, ncomp = 5)
# Réalisation de l'analyse PLS-DA
plsda_res5 <- plsda(X_train, Y_train, ncomp = 5)
plotIndiv(plsda_res5, group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title = "PLS-DA sur cinq composantes")
plotLoadings(plsda_res3, comp = 1, method = 'median', contrib = "max")
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
get.BER(confusion.mat)
confusion.mat
get.BER(confusion.mat)
??predict
test.predict <- predict(plsda_res3, X_test, dist = "mahalanobis.dist")
Prediction <- test.predict$class$max.dist[, 2]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
confusion.mat
get.BER(confusion.mat)
test.predict <- predict(plsda_res3, X_test, dist = "mahalanobis.dist")
test.predict <- predict(plsda_res3, X_test, dist = "mahalanobis.dist")
Prediction <- test.predict$class$max.dist[, 2]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
# Prédiction des classes des individus de l'échantillon de test
X_test <- as.matrix(df_test[,2:257])
Y_test <- df_test[, 1]
Y_test_pred <- predict(plsda_res3, newdata = X_test)$class
test.predict <- predict(plsda_res3, X_test, dist = "mahalanobis.dist")
test.predict <- predict(plsda_res3, X_test, dist = "mahalanobis.dist")
Prediction <- test.predict$class$max.dist[, 2]
Prediction <- test.predict$class$max.dist[, 2]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
# Prédiction des classes des individus de l'échantillon de test
X_test <- as.matrix(df_test[,2:257])
Y_test <- df_test[, 1]
Y_test_pred <- predict(plsda_res3, newdata = X_test)$class
# Calcul du taux de bonne classification
mean(Y_test_pred[["mahalanobis.dist"]] == df_test[,1])
test.predict <- predict(plsda_res3, X_test, dist = "all")
Prediction <- test.predict$class$max.dist[, 2]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
confusion.mat
get.BER(confusion.mat)
test.predict <- predict(plsda_res3, X_test, dist = "centroids.dist)
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
confusion.mat
get.BER(confusion.mat)
test.predict <- predict(plsda_res3, X_test, dist = "centroids.dist")
test.predict <- predict(plsda_res3, X_test, dist = "max.dist")
Prediction <- test.predict$class$max.dist[, 2]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
confusion.mat
get.BER(confusion.mat)
Prediction <- test.predict$class$max.dist[, 3]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
confusion.mat
get.BER(confusion.mat)
test.predict <- predict(plsda_res3, X_test, dist = "max.dist")
Prediction <- test.predict$class$max.dist[, 4]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
confusion.mat
test.predict <- predict(plsda_res3, X_test, dist = "max.dist")
Prediction <- test.predict$class$max.dist[, 4]
test.predict <- predict(plsda_res3, X_test, dist = "max.dist")
Prediction <- test.predict$class$max.dist[, 3]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
confusion.mat
get.BER(confusion.mat)
confusion.mat
# Sélection des variables x.1 à x.256
X <- df_train[, 2:257]
# Variables de réponse
y <- df_train[, 1]
# Division des données en ensembles d'apprentissage et de test
train_index <- sample(1:nrow(df_train), size = round(nrow(df_train) * 0.7), replace = FALSE)
train_set <- df_train[train_index, ]
test_set <- df_train[-train_index, ]
# Entraînement du modèle
model_lda <- lda(x = train_set[, 2:257], grouping = train_set[, 1])
# Matrice de confusion
table(lda_prediction$class, test_set[, 1])
# Sélection des variables x.1 à x.256
X <- df_train[, 2:257]
# Variables de réponse
y <- df_train[, 1]
# Division des données en ensembles d'apprentissage et de test
train_index <- sample(1:nrow(df_train), size = round(nrow(df_train) * 0.7), replace = FALSE)
train_set <- df_train[train_index, ]
test_set <- df_train[-train_index, ]
### Entraînement du modèle
```{r, echo=FALSE, eval=FALSE}
# Entraînement du modèle
model_lda <- lda(x = train_set[, 2:257], grouping = train_set[, 1])
# Prédiction des classes dans l'ensemble de test
lda_prediction <- predict(model_lda, newdata = test_set[, 2:257])
# Matrice de confusion
table(lda_prediction$class, test_set[, 1])
# Précision globale
mean(lda_prediction$class == test_set[,1])
#Validation croisée
res.geoDA <- geoDA(X_train,Y_train,validation="crossval")
library(mixOmics)
{
library(dplyr)
library(tidyverse)
library(ggplot2)
library(parallel)
library(FactoMineR)
library(factoextra)
library(caret)
library(klaR)
library(NbClust)
library(JLutils)
library(dendextend)
library(readr)
library(dplyr)
library(knitr)
library(DiscriMiner)
library(datawizard)
library(MASS)
library(pls)
library(plsRglm)
library(mdatools)
library(FactoMineR)
library(mixOmics)
}
# Séparation des données en matrices X et Y
X_train <- as.matrix(df_train[,2:257])
Y_train <- as.factor(df_train[,1])
# Réalisation de l'analyse PLS-DA
plsda_res5 <- plsda(X_train, Y_train, ncomp = 5)
plotIndiv(plsda_res5, group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title = "PLS-DA sur cinq composantes")
#Validation croisée
res.geoDA <- geoDA(X_train,Y_train,validation="crossval")
#Validation croisée
res.geoDA <- geoDA(X_train,Y_train,validation="crossval")
res.geoDA$error_rate #tx erreur en validation crois?e
# Réalisation de l'analyse PLS-DA
plsda_res3 <- plsda(X_train, Y_train, ncomp = 3)
# Visualisation des individus dans l'espace des trois premières composantes principales
plotIndiv(plsda_res3, comp = c(1,3), group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title =" PLS-DA sur trois composantes")
# Visualisation des individus dans l'espace des trois premières composantes principales
plotIndiv(plsda_res3, comp = c(1,3), group = Y_train, legend.title = "Phonème", ellipse = TRUE, legend = TRUE,
title =" PLS-DA sur trois composantes")
# Prédiction des classes des individus de l'échantillon de test
X_test <- as.matrix(df_test[,2:257])
Y_test <- df_test[, 1]
Y_test_pred <- predict(plsda_res3, newdata = X_test)$class
# Calcul du taux de bonne classification
mean(Y_test_pred[["mahalanobis.dist"]] == df_test[,1])
#| label: fig-plsda3
#| fig-cap: "Représentation graphique des PLS-DA"
#| fig-subcap:
#|   - "5 composantes"
#|   - "3 composantes"
#| layout-ncol: 3
# Visualisation des individus dans l'espace des trois premières composantes principales
plotLoadings(plsda_res3, comp = 1, method = 'median', contrib = "max")
# Séparation des données en matrices X et Y
X_train <- as.matrix(df_train[,2:257])
test.predict <- predict(plsda_res3, X_test, dist = "max.dist")
Prediction <- test.predict$class$max.dist[, 3]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
test.predict <- predict(plsda_res3, X_test, dist = "max.dist")
Prediction <- test.predict$class$max.dist[, 3]
confusion.mat <- get.confusion_matrix(truth = Y_test, predicted = Prediction)
confusion.mat
get.BER(confusion.mat)
confusion.mat
### Préparation des données
# Sélection des variables x.1 à x.256
X <- df_train[, 2:257]
# Variables de réponse
y <- df_train[, 1]
# Division des données en ensembles d'apprentissage et de test
train_index <- sample(1:nrow(df_train), size = round(nrow(df_train) * 0.7), replace = FALSE)
train_set <- df_train[train_index, ]
test_set <- df_train[-train_index, ]
```{r, echo=FALSE, eval=FALSE}
# Entraînement du modèle
model_lda <- lda(x = train_set[, 2:257], grouping = train_set[, 1])
#| label: fig-comp
#| fig-cap: "Contribution des individus aux trois composantes"
#| fig-subcap:
#|   - "Composante 1"
#|   - "Composante 2"
#|   - "Composante 3"
#| layout-ncol: 3
# Visualisation des individus dans l'espace des trois premières composantes principales
plotLoadings(plsda_res3, comp = 1, method = 'median', contrib = "max", title = "Composante 1")
plotLoadings(plsda_res3, comp = 2, method = 'median', contrib = "max", title = "Composante 2")
plotLoadings(plsda_res3, comp = 3, method = 'median', contrib = "max", title = "Composante 3")
#| label: fig-comp
#| fig-cap: "Contribution des individus aux trois composantes"
#| fig-subcap:
#|   - "Composante 1"
#|   - "Composante 2"
#|   - "Composante 3"
#| layout-ncol: 3
# Visualisation des individus dans l'espace des trois premières composantes principales
plotLoadings(plsda_res3, comp = 1, method = 'median', contrib = "max", title = "Composante 1")
plotLoadings(plsda_res3, comp = 2, method = 'median', contrib = "max", title = "Composante 2")
plotLoadings(plsda_res3, comp = 3, method = 'median', contrib = "max", title = "Composante 3")
plotLoadings(plsda_res3, comp = 2, method = 'median', contrib = "max", title = "Composante 2")
plotLoadings(plsda_res3, comp = 3, method = 'median', contrib = "max", title = "Composante 3")
#| label: fig-comp
#| fig-cap: "Contribution des individus aux trois composantes"
#| fig-subcap:
#|   - "Composante 1"
#|   - "Composante 2"
#|   - "Composante 3"
#| layout-ncol: 3
# Visualisation des individus dans l'espace des trois premières composantes principales
plotLoadings(plsda_res3, comp = 1, method = 'median', contrib = "max", title = "Composante 1")
#| label: fig-comp
#| fig-cap: "Contribution des individus aux trois composantes"
#| fig-subcap:
#|   - "Composante 1"
#|   - "Composante 2"
#|   - "Composante 3"
#| layout-ncol: 3
# Visualisation des individus dans l'espace des trois premières composantes principales
plotLoadings(plsda_res3, comp = 1, method = 'median', contrib = "max", title = "Composante 1")
plotLoadings(plsda_res3, comp = 2, method = 'median', contrib = "max", title = "Composante 2")
plotLoadings(plsda_res3, comp = 3, method = 'median', contrib = "max", title = "Composante 3")
{
library(dplyr)
library(tidyverse)
library(ggplot2)
library(parallel)
library(FactoMineR)
library(factoextra)
library(caret)
library(klaR)
library(NbClust)
library(JLutils)
library(dendextend)
library(readr)
library(dplyr)
library(knitr)
library(DiscriMiner)
library(datawizard)
library(MASS)
library(pls)
library(plsRglm)
library(mdatools)
library(FactoMineR)
library(mixOmics)
}
# Charger les données
df <- read.csv2("phoneme.csv", sep = ",")
# Si vous voulez déplacer la dernière colonne en première position :
df <- df %>%
relocate(names(df)[ncol(df)], .before = names(df)[1])
# Si vous voulez déplacer la colonne "g" en première position :
df <- df %>%
relocate(g, .before = names(df)[1])
# Convertir la colonne "g" en facteur
df$g <- as.factor(df$g)
# Convertir toutes les colonnes commençant par "x." en numérique
df <- df %>% mutate(across(starts_with("x."), as.numeric))
# Supprimer la colonne "row.names"
if ("row.names" %in% names(df)) {
df <- df[, -which(names(df) == "row.names")]
}
data_codebook(df, select = starts_with("g"))
# Création de train et test
df_train <- df[grepl("train", df$speaker), ]
df_train  = df_train[,-c(2)]
df_test <- df[grepl("test", df$speaker), ]
df_test  = df_test[,-c(2)]
df  = df[,-c(2)]
phoneme_groups <- split(df, df$g)
# Fonction pour calculer les statistiques descriptives
descriptive_stats <- function(data) {
# Sélectionner uniquement les colonnes numériques (supposons que les colonnes "x.1" à "x.256" sont numériques)
numeric_data <- data[, grepl("^x\\.", colnames(data))]
summary_stats <- data.frame(
mean = colMeans(numeric_data),
median = apply(numeric_data, 2, median),
sd = apply(numeric_data, 2, sd),
min = apply(numeric_data, 2, min),
max = apply(numeric_data, 2, max)
)
return(summary_stats)
}
# Appliquer la fonction aux groupes de phonèmes
phoneme_stats <- lapply(phoneme_groups, descriptive_stats)
# Convertissez les données en format long
#long_data <- reshape2::melt(numeric_data, id.vars = "g", variable.name = "variable", value.name = "value")
# Convertir la liste en data.frame
phoneme_stats_df <- do.call(rbind, phoneme_stats)
# Ajouter une colonne avec les noms des phonèmes
phoneme_stats_df$phoneme <- rep(names(phoneme_groups), each = nrow(phoneme_stats[[1]]))
# Ajouter une colonne avec les numéros de variable (périodogramme)
phoneme_stats_df$variable <- factor(rep(1:nrow(phoneme_stats[[1]]), length(unique(phoneme_stats_df$phoneme))), levels = 1:nrow(phoneme_stats[[1]]))
# Convertir les données en format long
long_phoneme_stats_df <- reshape2::melt(phoneme_stats_df, id.vars = c("phoneme", "variable"), variable.name = "statistic", value.name = "value")
# Ajouter une colonne avec les numéros de variable (périodogramme)
mean_data <- long_phoneme_stats_df[long_phoneme_stats_df$statistic == "mean", ]
# Extraire les moyennes de long_phoneme_stats_df
mean_data <- long_phoneme_stats_df[long_phoneme_stats_df$statistic == "mean",]
# Créer une colonne "variable" avec les numéros de variable (périodogramme) pour chaque phonème
mean_data$variable <- factor(rep(1:nrow(phoneme_stats[[1]]), length(unique(mean_data$phoneme))), levels = 1:nrow(phoneme_stats[[1]]))
# Créer une colonne "variable" avec les numéros de variable (périodogramme) pour chaque phonème
mean_data$variable <- factor(rep(1:nrow(phoneme_stats[[1]]), length(unique(mean_data$phoneme))), levels = 1:nrow(phoneme_stats[[1]]))
#| label: fig-period
#| fig-cap: "Log-periodogramme moyen par valeur et par phonème"
ggplot(mean_data, aes(x = variable, y = value, color = phoneme, group = phoneme)) +
geom_line() +
scale_x_discrete(breaks = seq(1, nrow(phoneme_stats[[1]]), by = 50), labels = seq(1, nrow(phoneme_stats[[1]]), by = 50)) +
theme_bw() +
labs(x = "Variable (Periodogram)",
y = "Valeur moyenne") +
theme(legend.position = "bottom",
panel.grid.major = element_blank(),
panel.grid.minor = element_blank())
res.pca <- PCA (df,scale.unit=TRUE,quali.sup=1,graph=FALSE)
res.pca <- PCA (df,scale.unit=TRUE,quali.sup=1,graph=FALSE)
res <- barplot(res.pca$eig[1:10],xlab="Dim.",ylab="Percentage of variance")
res <- barplot(res.pca$eig[1:10],xlab="Dim.",ylab="Percentage of variance")
plot(res.pca,choix="var")
#| label: fig-pca
#| fig-cap: "Projection graphique des individus"
FactoMineR::plot.PCA(res.pca,choix="ind",habillage=1, label="quali")
phoneme_counts <- table(df$g)
barplot(phoneme_counts, main = "Distribution des phonèmes")
data.cr <- scale(df_train[,-1],center=TRUE,scale=TRUE) # standardise les données en les centrant
data.dist <- dist(data.cr) #calcule la matrice des distances euclidiennes entre les observations standardisées.
par(mfrow=c(1,2))
height_data <- rev(data.hca$height)[1:30]
data.hca <- hclust(data.dist,method="ward.D2") #classification hiérarchique agglomérative en utilisant la méthode de Ward, qui minimise la somme des carrés intra-cluster.
par(mfrow=c(1,2))
height_data <- rev(data.hca$height)[1:30]
barplot_df <- data.frame(index = 1:length(height_data), height = height_data)
arbre <- hclust(data.dist,method="ward.D2")
plot(arbre)
plot(arbre)
inertie <- sort(arbre$height, decreasing = TRUE)
inertie <- sort(arbre$height, decreasing = TRUE)
plot(inertie[1:20], type = "s", xlab = "Nombre de classes", ylab = "Inertie")
ggplot(barplot_df, aes(x = index, y = height)) +
labs(x = "Index",
y = "Va") +
geom_bar(stat = "identity", fill = "royalblue") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
theme_bw()
ggplot(color_branches(arbre, k = 3), labels = FALSE)
res.cutree <- cutree(data.hca, 3) # coupe l'arbre de classification hiérarchique en 3 clusters.
centers <- aggregate(data.cr, by=list(res.cutree), FUN=mean) # calcule les centres de gravité (moyennes) des 3 clusters obtenus à partir de la classification hiérarchique.
centers <- centers[,-1] # supprime la première colonne des centres, qui contient les identifiants de cluster.
data.kmeans <- kmeans(data.cr, centers=centers, algorithm="MacQueen") # clustering k-means en utilisant les centres de gravité calculés précédemment comme centres initiaux, avec l'algorithme de MacQueen.
data.kmeans.alea <- kmeans(data.cr, centers=centers, nstart = 50, iter.max = 50, algorithm="MacQueen") # clustering k-means avec les centres initiaux, mais en effectuant 50 itérations maximales et en choisissant la meilleure solution parmi 50 essais aléatoires.
data.tot <- cbind(as.data.frame(df_train), Clas = as.factor(data.kmeans$cluster))  # ajoute une colonne "Clas" à l'ensemble de données original, contenant les résultats du clustering k-means.
#| label: tbl-clust
#| tbl-cap: "Iris Data"
par(mfrow = c(1,3))
kable(table(data.tot[,c(1,258)])) # Tableau croisé des clusters et de la première variable (colonne 1) de l'ensemble de données.
chisq.test(table(data.tot[,c(1,258)])) # test du chi-carré d'indépendance pour déterminer s'il existe une association significative entre la première variable et les clusters.
# Recalcul du tableau de répartition des phonèmes dans les différents clusters
tbl_clust <- table(data.tot[,c(1,258)])
# Calcul des pourcentages de chaque phonème dans chaque cluster
pourcentages <- round(prop.table(tbl_clust, margin = 2) * 100, 1)
# Ajout d'une colonne avec les pourcentages de chaque phonème dans chaque cluster
tbl_clust <- cbind(tbl_clust, pourcentages)
# Affichage du tableau mis à jour
tbl_clust
### Préparation des données
# Sélection des variables x.1 à x.256
X <- df_train[, 2:257]
# Variables de réponse
y <- df_train[, 1]
# Division des données en ensembles d'apprentissage et de test
train_index <- sample(1:nrow(df_train), size = round(nrow(df_train) * 0.7), replace = FALSE)
train_set <- df_train[train_index, ]
test_set <- df_train[-train_index, ]
# Entraînement du modèle
model_lda <- lda(x = train_set[, 2:257], grouping = train_set[, 1])
# Prédiction des classes dans l'ensemble de test
lda_prediction <- predict(model_lda, newdata = test_set[, 2:257])
### Mesures de performances
# Matrice de confusion
table(lda_prediction$class, test_set[, 1])
# Précision globale
mean(lda_prediction$class == test_set[,1])
# Obtention des coefficients du modèle
coef_lda <- coef(model_lda)
# Obtention des scores d'importance des variables
importance <- abs(model_lda$scaling)^2
# Création d'un dataframe contenant les coefficients et leur importance relative
coef_df <- data.frame(variable = colnames(train_set[, 2:257]),
coef = coef_lda[,1],
importance = importance[,1])
# Tri du dataframe par importance décroissante
sorted_coef_df <- coef_df[order(-coef_df$importance),]
# Affichage des 10 premières variables les plus discriminantes
head(sorted_coef_df, 10)
View(sorted_coef_df)
# Affichage des 10 premières variables les plus discriminantes
kable(head(sorted_coef_df, 10))
